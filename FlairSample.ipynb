{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.5-py3-none-any.whl (334 kB)\n",
      "\u001b[K     |████████████████████████████████| 334 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting bpemb>=0.2.9\n",
      "  Downloading bpemb-0.3.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: torch>=1.1.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (2.8.0)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.8.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (3.8.0)\n",
      "Requirement already satisfied: regex in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (2019.8.19)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (0.23.1)\n",
      "Collecting pytest>=5.3.2\n",
      "  Downloading pytest-5.4.3-py3-none-any.whl (248 kB)\n",
      "\u001b[K     |████████████████████████████████| 248 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (3.2.1)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-1.6.0.tar.gz (29 kB)\n",
      "Collecting transformers>=2.10.0\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "\u001b[K     |████████████████████████████████| 674 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "\u001b[K     |████████████████████████████████| 788 kB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tabulate in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (0.8.6)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from flair) (4.46.0)\n",
      "Collecting hyperopt>=0.1.1\n",
      "  Using cached hyperopt-0.2.4-py2.py3-none-any.whl (964 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: requests in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from bpemb>=0.2.9->flair) (2.22.0)\n",
      "Requirement already satisfied: numpy in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from bpemb>=0.2.9->flair) (1.18.4)\n",
      "Requirement already satisfied: sentencepiece in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from bpemb>=0.2.9->flair) (0.1.85)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->flair) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn>=0.21.3->flair) (0.13.2)\n",
      "Requirement already satisfied: py>=1.5.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from pytest>=5.3.2->flair) (1.8.0)\n",
      "Requirement already satisfied: wcwidth in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from pytest>=5.3.2->flair) (0.1.7)\n",
      "Requirement already satisfied: packaging in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from pytest>=5.3.2->flair) (19.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from pytest>=5.3.2->flair) (19.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from pytest>=5.3.2->flair) (0.23)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from pytest>=5.3.2->flair) (0.13.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from pytest>=5.3.2->flair) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (2.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: sacremoses in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from transformers>=2.10.0->flair) (0.0.38)\n",
      "Requirement already satisfied: filelock in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from transformers>=2.10.0->flair) (3.0.12)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp37-cp37m-macosx_10_10_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
      "Requirement already satisfied: cloudpickle in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair) (1.2.2)\n",
      "Requirement already satisfied: future in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair) (0.17.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from requests->bpemb>=0.2.9->flair) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from requests->bpemb>=0.2.9->flair) (1.25.9)\n",
      "Requirement already satisfied: boto3 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim>=3.4.0->flair) (1.11.9)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->flair) (46.1.3)\n",
      "Requirement already satisfied: click in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers>=2.10.0->flair) (7.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from networkx>=2.2->hyperopt>=0.1.1->flair) (4.4.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim>=3.4.0->flair) (0.3.2)\n",
      "Collecting botocore<1.15.0,>=1.14.9\n",
      "  Downloading botocore-1.14.17-py2.py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim>=3.4.0->flair) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/rahul.b.sarkar/opt/anaconda3/lib/python3.7/site-packages (from botocore<1.15.0,>=1.14.9->boto3->smart-open>=1.7.0->gensim>=3.4.0->flair) (0.15.2)\n",
      "Building wheels for collected packages: segtok, langdetect, sqlitedict, mpld3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for segtok (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25019 sha256=57e752b423e64c940eac66b4c6730de1d1e2ef04bec558ecd9a5d7b37e726db9\n",
      "  Stored in directory: /Users/rahul.b.sarkar/Library/Caches/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.8-py3-none-any.whl size=993191 sha256=05c7212034944b482725a7064c5b864ba4279355358caab286504d9f31972793\n",
      "  Stored in directory: /Users/rahul.b.sarkar/Library/Caches/pip/wheels/59/f6/9d/85068904dba861c0b9af74e286265a08da438748ee5ae56067\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-py3-none-any.whl size=14688 sha256=5b6d4d5bc8723a13ad1c49c7687ed12e760fd65beca4b28f44c90c4fc730b1a2\n",
      "  Stored in directory: /Users/rahul.b.sarkar/Library/Caches/pip/wheels/58/dd/2c/0a57aadf6a7f26bec0af66d742c50af74d11967780f0bb7a7d\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116678 sha256=b5b7cdd176d7295446f100bc36c602207bafb72e302c18007b36f1d4cd1a2f59\n",
      "  Stored in directory: /Users/rahul.b.sarkar/Library/Caches/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
      "Successfully built segtok langdetect sqlitedict mpld3\n",
      "\u001b[31mERROR: pytest-pylint 0.14.1 requires pylint>=1.4.5, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: sentence-transformers 0.2.5 has requirement transformers==2.3.0, but you'll have transformers 2.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 0.2.0 has requirement spacy==2.2.0, but you'll have spacy 2.2.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 0.2.0 has requirement transformers==2.3.0, but you'll have transformers 2.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.56 has requirement botocore==1.16.6, but you'll have botocore 1.14.17 which is incompatible.\u001b[0m\n",
      "Installing collected packages: segtok, deprecated, bpemb, langdetect, pytest, sqlitedict, tokenizers, transformers, mpld3, hyperopt, flair, botocore\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 5.2.1\n",
      "    Uninstalling pytest-5.2.1:\n",
      "      Successfully uninstalled pytest-5.2.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.3.0\n",
      "    Uninstalling transformers-2.3.0:\n",
      "      Successfully uninstalled transformers-2.3.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.16.6\n",
      "    Uninstalling botocore-1.16.6:\n",
      "      Successfully uninstalled botocore-1.16.6\n",
      "Successfully installed botocore-1.14.17 bpemb-0.3.0 deprecated-1.2.10 flair-0.5 hyperopt-0.2.4 langdetect-1.0.8 mpld3-0.3 pytest-5.4.3 segtok-1.5.10 sqlitedict-1.6.0 tokenizers-0.7.0 transformers-2.11.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/rahul.b.sarkar/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 20:41:05,858 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/NER-conll03-english/en-ner-conll03-v0.4.pt not found in cache, downloading to /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpi1rvvg6l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [01:06<00:00, 6479174.20B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 20:42:13,296 copying /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpi1rvvg6l to cache at /Users/rahul.b.sarkar/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 20:42:13,993 removing temp file /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpi1rvvg6l\n",
      "2020-06-08 20:42:14,019 loading file /Users/rahul.b.sarkar/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sentence: \"I love Berlin .\"   [− Tokens: 4  − Token-Labels: \"I love Berlin <S-LOC> .\"]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('I love Berlin .')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"I love Berlin .\"   [− Tokens: 4  − Token-Labels: \"I love Berlin <S-LOC> .\"]\n",
      "The following NER tags are found:\n",
      "Span [3]: \"Berlin\"   [− Labels: LOC (0.9992)]\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print('The following NER tags are found:')\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging with Pre-Trained Sequence Tagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:15:55,154 loading file /Users/rahul.b.sarkar/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George <B-PER> Washington <E-PER> went to Washington <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('George Washington went to Washington .')\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Annotated Spans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span [1,2]: \"George Washington\"   [− Labels: PER (0.9968)]\n",
      "Span [5]: \"Washington\"   [− Labels: LOC (0.9994)]\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'George Washington went to Washington .', 'labels': [], 'entities': [{'text': 'George Washington', 'start_pos': 0, 'end_pos': 17, 'labels': [PER (0.9968)]}, {'text': 'Washington', 'start_pos': 26, 'end_pos': 36, 'labels': [LOC (0.9994)]}]}\n"
     ]
    }
   ],
   "source": [
    "print(sentence.to_dict(tag_type='ner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging a German Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:18:25,204 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/release-de-ner-0/de-ner-conll03-v0.4.pt not found in cache, downloading to /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmp2ewtc0ha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1512392120/1512392120 [03:58<00:00, 6335148.08B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:22:24,715 copying /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmp2ewtc0ha to cache at /Users/rahul.b.sarkar/.flair/models/de-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:22:27,367 removing temp file /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmp2ewtc0ha\n",
      "2020-06-10 02:22:27,759 loading file /Users/rahul.b.sarkar/.flair/models/de-ner-conll03-v0.4.pt\n",
      "George <B-PER> Washington <E-PER> ging nach Washington <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "tagger = SequenceTagger.load('de-ner')\n",
    "\n",
    "# make German sentence\n",
    "sentence = Sentence('George Washington ging nach Washington .')\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Multilingual Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:22:51,341 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/release-dodekapos-512-l2-multi/pos-multi-v0.1.pt not found in cache, downloading to /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpdrmenaed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314055714/314055714 [00:37<00:00, 8410938.56B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:23:29,414 copying /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpdrmenaed to cache at /Users/rahul.b.sarkar/.flair/models/pos-multi-v0.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:23:29,886 removing temp file /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpdrmenaed\n",
      "2020-06-10 02:23:29,906 loading file /Users/rahul.b.sarkar/.flair/models/pos-multi-v0.1.pt\n",
      "George <PROPN> Washington <PROPN> went <VERB> to <ADP> Washington <PROPN> . <PUNCT> Dort <ADV> kaufte <VERB> er <PRON> einen <DET> Hut <NOUN> . <PUNCT>\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "tagger = SequenceTagger.load('pos-multi')\n",
    "\n",
    "# text with English and German sentences\n",
    "sentence = Sentence('George Washington went to Washington . Dort kaufte er einen Hut .')\n",
    "\n",
    "# predict PoS tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Frame Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:26:08,657 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/release-frame-1/en-frame-ontonotes-v0.4.pt not found in cache, downloading to /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpaj6lhw_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290521503/290521503 [00:47<00:00, 6053749.39B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:26:57,410 copying /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpaj6lhw_s to cache at /Users/rahul.b.sarkar/.flair/models/en-frame-ontonotes-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 02:26:57,880 removing temp file /var/folders/kf/0_7nrl3505gd3q0f0mky79k80000gn/T/tmpaj6lhw_s\n",
      "2020-06-10 02:26:57,898 loading file /Users/rahul.b.sarkar/.flair/models/en-frame-ontonotes-v0.4.pt\n",
      "George <_> returned <return.01> to <_> Berlin <_> to <_> return <return.02> his <_> hat <_> . <_>\n",
      "He <_> had <have.03> a <_> look <look.01> at <_> different <_> hats <_> . <_>\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "tagger = SequenceTagger.load('frame')\n",
    "\n",
    "# make English sentence\n",
    "sentence_1 = Sentence('George returned to Berlin to return his hat .')\n",
    "sentence_2 = Sentence('He had a look at different hats .')\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence_1)\n",
    "tagger.predict(sentence_2)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "print(sentence_1.to_tagged_string())\n",
    "print(sentence_2.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
